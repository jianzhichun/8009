{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/jianzhichun/8009/blob/main/group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "cell_id": "ac374e7f-a23a-4f09-981a-66935ea4c86a",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 90
   }
  },
  {
   "cell_type": "code",
   "source": "import torch\nprint(torch.__version__)",
   "metadata": {
    "cell_id": "dcfb64dbc00b4a4893720e17454baed6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bd665810",
    "execution_start": 1649551438720,
    "execution_millis": 1145,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 174.25
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.8/py/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n1.11.0+cu102\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kc4kwXDZViGT",
    "outputId": "f1ee2bf6-2b62-4222-da6b-c687dffbc8e2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cell_id": "00001-3f3ede1c-d802-4cbb-a78d-f5ece0459f55",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "364c6588",
    "execution_start": 1649551439912,
    "execution_millis": 21748,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 376.90625
   },
   "source": "# !pip uninstall torch-scatter\n# !pip uninstall torch-sparse\n!pip install -q torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cu102.html\n\n#if you are doing it on your local machine, pls figure out your pytorch version and the cuda version and modify the following code accordingly.\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu102.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu102.html\n!pip install -q networkx",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.0.4 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.0.4 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.0.4 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.0.4 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J831nLYwViGU",
    "cell_id": "00002-e892519d-e799-4a73-bce1-da94a6f9e4d3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2ab571c1",
    "execution_start": 1649551461680,
    "execution_millis": 2854,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 207
   },
   "source": "import torch\nimport os\nimport pandas as pd\nfrom torch_geometric.data import InMemoryDataset, Data, download_url, extract_zip\nfrom torch_geometric.utils.convert import to_networkx\nimport networkx as nx\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "To create the dataset we need to convert the raw information into a ```Data``` object (a graph) in PyG.\n\nThe first step is to load the csv files, this can be done manually or using some data library as Pandas:",
   "metadata": {
    "id": "P-yaHJk1ViGU",
    "cell_id": "00003-80ce4e77-67cb-423f-8bbe-55d14f9e633e",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 111.1875
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Open Graph Benchmark datasets",
   "metadata": {
    "id": "p4x0tTViViGY",
    "cell_id": "00004-8556d921-2c40-44f4-9cfc-926f16be2aad",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "markdown",
   "source": "Open Graph Benchmark is available as a python library, to install it just run\n\n```pip install ogb```",
   "metadata": {
    "id": "DmLTZEZ4ViGY",
    "cell_id": "00005-5d693ee2-fe1e-4d15-81f9-3842f6294c12",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 88.78125
   }
  },
  {
   "cell_type": "markdown",
   "source": "OGB allows to load a dataset in three ways: for PyG applications, for DGL (Deep Graph Library, another widely used tool for GNNs in python) and in an 'agnostic' manner. There is a naming convention to load a dataset, depending on the task an the dataset name:\n\n    ogbn-[name]: for node tasks\n    ogbg-[name]: for graph tasks\n    ogbl-[name]: for link tasks",
   "metadata": {
    "id": "PqYtXEmjViGY",
    "cell_id": "00006-9766d515-6983-45be-a5ab-668b8dd20387",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 193.5625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mP09G9dIWuWK",
    "outputId": "a0055758-bcac-4817-f3ee-42b1a58da2d6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cell_id": "00007-febdbf8b-08d0-421b-ba80-006865e33bea",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dc3025c9",
    "execution_start": 1649551464549,
    "execution_millis": 6328,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 519.09375
   },
   "source": "!pip install ogb",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: ogb in /usr/local/lib/python3.8/dist-packages (1.3.3)\nRequirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (0.2.1)\nRequirement already satisfied: six>=1.12.0 in /shared-libs/python3.8/py-core/lib/python3.8/site-packages (from ogb) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /usr/lib/python3/dist-packages (from ogb) (1.25.8)\nRequirement already satisfied: scikit-learn>=0.20.0 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from ogb) (1.0.2)\nRequirement already satisfied: tqdm>=4.29.0 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from ogb) (4.63.0)\nRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.21.3)\nRequirement already satisfied: torch>=1.6.0 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from ogb) (1.11.0)\nRequirement already satisfied: pandas>=0.24.0 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from ogb) (1.2.5)\nRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from outdated>=0.2.0->ogb) (2.22.0)\nRequirement already satisfied: littleutils in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from scikit-learn>=0.20.0->ogb) (1.8.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\nRequirement already satisfied: pytz>=2017.3 in /shared-libs/python3.8/py/lib/python3.8/site-packages (from pandas>=0.24.0->ogb) (2022.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /shared-libs/python3.8/py-core/lib/python3.8/site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.0.4 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wyb9bVD-ViGY",
    "cell_id": "00008-e36b477e-fb02-4965-9296-72b756aa4ddc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b366a458",
    "execution_start": 1649551470883,
    "execution_millis": 1492,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "from ogb.nodeproppred import PygNodePropPredDataset\n#from ogb.graphproppred import PygGraphPropPredDataset\n#from ogb.linkproppred import PygLinkPropPredDataset",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l1zadVohViGY",
    "cell_id": "00009-6e8ee457-dae9-4b86-b427-111af038ca39",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4523e923",
    "execution_start": 1649551472378,
    "execution_millis": 18207,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 456.375
   },
   "source": "import torch_geometric.transforms as T\n\ndataset_name = 'ogbn-arxiv'\ndataset = PygNodePropPredDataset(name = dataset_name, root='data', transform=T.ToSparseTensor()) \n\nsplit_idx = dataset.get_idx_split()\ntrain_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\ngraph = dataset[0] ",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\nDownloaded 0.08 GB: 100%|██████████| 81/81 [00:04<00:00, 18.48it/s]\nExtracting data/arxiv.zip\nProcessing...\nLoading necessary files...\nThis might take a while.\nProcessing graphs...\n100%|██████████| 1/1 [00:00<00:00, 4297.44it/s]\nConverting graphs into PyG objects...\n100%|██████████| 1/1 [00:00<00:00, 4140.48it/s]Saving...\n\nDone!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### You have to write a short proposal for me to review.",
   "metadata": {
    "id": "LFz1-mfzcw9r",
    "cell_id": "00010-6ce9d0dc-2043-4cd2-8fe9-a8f5911ce706",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "markdown",
   "source": "In this project, I will try to use Graph Neural Networks to predict a collection of papers from arXiv. We will build our first graph neural network using PyTorch Geometric and apply it to node attribute prediction (node classification). And We will build a graph neural network using the GCN operator.",
   "metadata": {
    "id": "lq7xWYoQeeEn",
    "cell_id": "00011-95a86b9d-1bb9-4f4c-8576-cf616795f6f6",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 97.1875
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Describe the problem and the dataset clearly.",
   "metadata": {
    "id": "FDn_SH5Gc48i",
    "cell_id": "00012-1f6894e2-2ccd-428b-bf42-ce6378735ca7",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### What’s the input, what’s the output?",
   "metadata": {
    "id": "z_pyb3JldFBk",
    "cell_id": "00013-c3afe2fb-9d2a-4dbc-8f85-13717376263d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0BuhoHedaLS",
    "outputId": "0484c26e-8f54-46c8-cb16-7bd0afcd5edb",
    "cell_id": "00014-7dea8c2c-1908-4be4-a2e9-11525f7826f3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f1e2aa8c",
    "execution_start": 1649551490604,
    "execution_millis": 30,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 149.71875
   },
   "source": "print(f'Input: the graph data with {graph.num_features} features')\nprint(f'Output: the accuracy of our prediction model')",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "Input: the graph data with 128 features\nOutput: the accuracy of our prediction model\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### Why is the problem important?",
   "metadata": {
    "id": "S71lrYyGdNA4",
    "cell_id": "00015-f2e65b2c-1c4d-45a9-bc31-6dec5ee44f23",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "markdown",
   "source": "Finding an accurate model can help us classify papers and predict the properties of papers.",
   "metadata": {
    "id": "cQNrCJR_hWIn",
    "cell_id": "00016-5b68f283-ed4c-4dff-8b0f-a57fa2087b9d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### What’s the most popular baseline method?",
   "metadata": {
    "id": "G9kR7yLRdPtf",
    "cell_id": "00017-fcf00c31-cd60-4358-96e0-dc7da982249d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "markdown",
   "source": "GCN, Node2Vec, MLP...",
   "metadata": {
    "id": "VZoOWF8iiBQu",
    "cell_id": "00018-96a9c904-e4c7-480a-97dd-5a4563e3068f",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### What’s the evaluation metric?",
   "metadata": {
    "id": "Nb4Pyk1adS-0",
    "cell_id": "00019-190d6ec6-04fb-41fe-89c0-2927d2e18ea9",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "markdown",
   "source": "* The standardized evaluation protocol from ogb.\n* The accuracy, precision, recall and F1-score",
   "metadata": {
    "id": "P9Y-ldY0iQKj",
    "cell_id": "00020-727169ea-169a-4f34-ad1e-966ae6e3620f",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 94.78125
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Prediction Model",
   "metadata": {
    "id": "JSqWrgtBj521",
    "cell_id": "00021-3d0309f3-43bf-4045-aecd-1826962250f9",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### GCN",
   "metadata": {
    "id": "sOvWCSxCkAZ9",
    "cell_id": "00022-2fdd9047-7d1e-4141-b016-23389ded68a5",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2VZVtcLJjjoJ",
    "cell_id": "00023-b6cc751d-c104-4858-8f16-8684b8566af8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3fe994e7",
    "execution_start": 1649551490660,
    "execution_millis": 607,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1539
   },
   "source": "import torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n                 dropout, return_embeds=False):\n        # TODO: Implement this function that initializes self.convs, \n        # self.bns, and self.softmax.\n\n        super(GCN, self).__init__()\n\n        # A list of GCNConv layers\n        self.convs = torch.nn.ModuleList(\n            [GCNConv(in_channels=input_dim, out_channels=hidden_dim)] +\n            [GCNConv(in_channels=hidden_dim, out_channels=hidden_dim)                             \n                for i in range(num_layers-2)] + \n            [GCNConv(in_channels=hidden_dim, out_channels=output_dim)]    \n        )\n\n        # A list of 1D batch normalization layers\n        self.bns = torch.nn.ModuleList([\n            torch.nn.BatchNorm1d(num_features=hidden_dim) \n                for i in range(num_layers-1)\n        ])\n        \n\n        # The log softmax layer\n        self.softmax = torch.nn.LogSoftmax()\n\n        ############# Your code here ############\n        ## Note:\n        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n        ## 2. self.convs has num_layers GCNConv layers\n        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n        ## 'out_channels'. More information please refer to the documentation:\n        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n        ## More information please refer to the documentation: \n        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n        ## (~10 lines of code)\n\n        #########################################\n\n        # Probability of an element to be zeroed\n        self.dropout = dropout\n\n        # Skip classification layer and return node embeddings\n        self.return_embeds = return_embeds\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n        for bn in self.bns:\n            bn.reset_parameters()\n\n    def forward(self, x, adj_t):\n        # TODO: Implement this function that takes the feature tensor x,\n        # edge_index tensor adj_t and returns the output tensor as\n        # shown in the figure.\n        for conv, bn in zip(self.convs[:-1], self.bns):\n            x1 = F.relu(bn(conv(x, adj_t)))\n            if self.training:\n                x1 = F.dropout(x1, p=self.dropout)\n            x = x1\n        x = self.convs[-1](x, adj_t)\n        out = x if self.return_embeds else self.softmax(x)\n\n        ############# Your code here ############\n        ## Note:\n        ## 1. Construct the network as showing in the figure\n        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n        ## More information please refer to the documentation:\n        ## https://pytorch.org/docs/stable/nn.functional.html\n        ## 3. Don't forget to set F.dropout training to self.training\n        ## 4. If return_embeds is True, then skip the last softmax layer\n        ## (~7 lines of code)\n\n        #########################################\n\n        return out",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### GGNN",
   "metadata": {
    "id": "vi0sL_TukVKD",
    "cell_id": "00024-8e5a118f-c735-4819-9d78-8b527e5ae3d6",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9Gvum4OXj_LN",
    "cell_id": "00025-59f0f9d2-628c-4308-9fe1-134c1f2ea409",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cf445ff5",
    "execution_start": 1649551491298,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "# TODO",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Train & Evaluation",
   "metadata": {
    "id": "PdaRzKmakY5r",
    "cell_id": "00026-b81350e0-6429-4ee3-9370-77b83df6394d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jno4FZw8kdfr",
    "cell_id": "00027-4463038d-2c00-41cf-a7e9-3d956615b5e9",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c2dc64c7",
    "execution_start": 1649551491344,
    "execution_millis": 44,
    "owner_user_id": "7ccf710a-f1bc-4549-abc5-b2590c7d3541",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1071
   },
   "source": "from ogb.nodeproppred import Evaluator\ndef train(model, data, train_idx, optimizer, loss_fn):\n    # TODO: Implement this function that trains the model by \n    # using the given optimizer and loss_fn.\n    model.train()\n    loss = 0\n\n    ############# Your code here ############\n    ## Note:\n    ## 1. Zero grad the optimizer\n    ## 2. Feed the data into the model\n    ## 3. Slicing the model output and label by train_idx\n    ## 4. Feed the sliced output and label to loss_fn\n    ## (~4 lines of code)\n    optimizer.zero_grad()\n    out = model(data.x, data.adj_t)\n    loss = loss_fn(out[train_idx], data.y[train_idx].reshape(-1))\n\n    #########################################\n\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    # TODO: Implement this function that tests the model by \n    # using the given split_idx and evaluator.\n    model.eval()\n\n    # The output of model on all data\n    out = None\n\n    ############# Your code here ############\n    ## (~1 line of code)\n    ## Note:\n    ## 1. No index slicing here\n    out = model(data.x, data.adj_t)\n    #########################################\n\n    y_pred = out.argmax(dim=-1, keepdim=True)\n\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n\n    return train_acc, valid_acc, test_acc",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Apply & Compare",
   "metadata": {
    "id": "2ziyqvjCkvXY",
    "cell_id": "00028-e3689cbc-f4e0-4f18-b70b-93183fbb5026",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### GCN",
   "metadata": {
    "id": "uUvA3wz3k8PH",
    "cell_id": "00029-b6ba895a-d22b-49a2-81d7-3c6ae85a9585",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ0Gm1yrk43Y",
    "outputId": "38da1805-6878-4018-82cb-20066ef1a243",
    "cell_id": "00030-5b2660d0-c39e-4882-bb16-bf14179b6aaf",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a691c1c0",
    "execution_start": 1649551585788,
    "owner_user_id": "cab3b104-0ce6-4e64-b13a-0c3b6085828b",
    "execution_millis": 450149,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 931
   },
   "source": "model = GCN(graph.num_features, 256, dataset.num_classes, 3, 0.5).to('cpu')\nevaluator = Evaluator(name='ogbn-arxiv')\nimport copy\n\n# reset the parameters to initial random value\nmodel.reset_parameters()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nloss_fn = F.nll_loss\n\nbest_model = None\nbest_valid_acc = 0\n\n# TODO\nepochs = 10\nfor epoch in range(1, 1 + epochs):\n  loss = train(model, graph, train_idx, optimizer, loss_fn)\n  result = test(model, graph, split_idx, evaluator)\n  train_acc, valid_acc, test_acc = result\n  if valid_acc > best_valid_acc:\n    best_valid_acc = valid_acc\n    best_model = copy.deepcopy(model)\n  print(f'Epoch: {epoch:02d}, '\n        f'Loss: {loss:.4f}, '\n        f'Train: {100 * train_acc:.2f}%, '\n        f'Valid: {100 * valid_acc:.2f}% '\n        f'Test: {100 * test_acc:.2f}%')\nbest_result = test(best_model, graph, split_idx, evaluator)\ntrain_acc, valid_acc, test_acc = best_result\nprint(f'Best model: '\n  f'Train: {100 * train_acc:.2f}%, '\n  f'Valid: {100 * valid_acc:.2f}% '\n  f'Test: {100 * test_acc:.2f}%')",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": "<ipython-input-8-9850d9a335a7>:68: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  out = x if self.return_embeds else self.softmax(x)\nEpoch: 01, Loss: 4.2554, Train: 10.05%, Valid: 12.15% Test: 11.58%\nEpoch: 02, Loss: 2.7206, Train: 23.43%, Valid: 18.21% Test: 17.07%\nEpoch: 03, Loss: 2.4676, Train: 26.59%, Valid: 26.00% Test: 28.02%\nEpoch: 04, Loss: 2.3044, Train: 28.43%, Valid: 30.05% Test: 32.85%\nEpoch: 05, Loss: 2.2083, Train: 29.96%, Valid: 32.99% Test: 35.54%\nEpoch: 06, Loss: 2.1166, Train: 33.05%, Valid: 36.88% Test: 38.34%\nEpoch: 07, Loss: 2.0528, Train: 36.79%, Valid: 40.66% Test: 40.77%\nEpoch: 08, Loss: 1.9979, Train: 40.03%, Valid: 43.57% Test: 42.26%\nEpoch: 09, Loss: 1.9492, Train: 42.67%, Valid: 45.93% Test: 43.26%\nEpoch: 10, Loss: 1.9101, Train: 45.13%, Valid: 47.79% Test: 44.38%\nBest model: Train: 45.13%, Valid: 47.79% Test: 44.38%\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ff505976-6683-4cd6-82f7-fd1506e405f3' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "colab": {
   "name": "load_data_from_ogbn_arxiv.ipynb",
   "provenance": [],
   "toc_visible": true,
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "deepnote_notebook_id": "3d0de2b8-b983-4049-9473-027434fb26e8",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}